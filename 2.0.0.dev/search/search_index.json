{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#spatial-transcriptomics-st-pipeline","title":"Spatial Transcriptomics (ST) Pipeline","text":"<p>The ST Pipeline provides the tools, algorithms and scripts needed to process and analyze the raw data generated with Spatial Transcriptomics or Visium in FASTQ format to generate datasets for down-stream analysis.</p> <p>The ST Pipeline can also be used to process single cell/nuclei RNA-seq data as long as a file with molecular <code>barcodes</code> identifying each cell is provided (same template as the files in the folder \"ids\").</p> <p>The ST Pipeline can also be used to process bulk RNA-seq data, in this case the barcodes file is not required.</p> <p>The ST Pipeline has been optimized for speed, robustness and it is very easy to use with many parameters to adjust all the settings. The ST Pipeline is fully parallel and it has constant memory use. The ST Pipeline allows to skip any of the main steps and provides multiple customization options. The ST Pipeline allows to use either the genome or the transcriptome as reference.</p> <p>Basically what the ST pipeline does (default mode) is:</p> <ul> <li>Quality trimming step (read 1 and read 2):</li> <li>Remove low quality bases</li> <li>Sanity check (reads same length, reads order, etc..)</li> <li>Check quality UMI</li> <li>Remove artifacts (PolyT, PolyA, PolyG, PolyN and PolyC) of user defined length</li> <li>Check for AT and GC content</li> <li>Discard reads with a minimum number of bases of that failed any of the checks above</li> <li>Contamimant filter step (e.x. rRNA genome) (Optional)</li> <li>Mapping with STAR step (only read 2) (Optional)</li> <li>Demultiplexing with Taggd step (only read 1) (Optional)</li> <li>Keep reads (read 2) that contain a valid barcode and are correctly mapped</li> <li>Annotate the reads to the reference (Optional)</li> <li>Group annotated reads by barcode (spot position), gene and genomic location (with an offset) to get a read count</li> <li>In the grouping/counting only unique molecules (UMIs) are kept (Optional)</li> </ul> <p>You can see a graphical more detailed description of the workflow in the documents <code>workflow.pdf</code> and <code>workflow_extended.pdf</code></p> <p>The output dataset is a matrix of counts (genes as columns, spots as rows) in TSV format. The ST pipeline will also output a log file with useful stats and information.</p>"},{"location":"#installation","title":"Installation","text":"<p>For users see install</p> <p>For developers contributing</p>"},{"location":"#usage","title":"Usage","text":"<p>See usage</p>"},{"location":"#authors","title":"Authors","text":"<p>See authors</p>"},{"location":"#license","title":"License","text":"<p>The ST pipeline is open source under the MIT license which means that you can use it, change it and re-distribute but you must always refer to our license (see LICENSE).</p>"},{"location":"#credits","title":"Credits","text":"<p>If you use the ST Pipeline, please refer its publication: ST Pipeline: An automated pipeline for spatial mapping of unique transcripts Oxford BioInformatics 10.1093/bioinformatics/btx211</p>"},{"location":"#example-dataset","title":"Example dataset","text":"<p>You can see a real dataset obtained from the public data from the following publication (http://science.sciencemag.org/content/353/6294/78) in the folder called \"data\".</p>"},{"location":"#contact","title":"Contact","text":"<p>For questions, bugs, feedback, etc.. you can contact:</p> <p>Jose Fernandez Navarro jc.fernandez.navarro@gmail.com</p>"},{"location":"api/","title":"Modules","text":"<p>:: ../stpipeline</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":""},{"location":"changelog/#version-200","title":"Version 2.0.0","text":"<ul> <li>Refactor code to modern Python (black, mypy, isort)</li> <li>Added Github Actions</li> <li>Added pre-commit hooks</li> <li>Changed the build configuration to Poetry</li> <li>Added a Docker container</li> <li>Changed tests to pytest</li> <li>Updated versions of dependencies</li> <li>Performed code optimizations, refactorings and clean ups</li> <li>Added more tests for almost full coveragge</li> <li>Bumped taggd to 0.4.0</li> <li>Changed documentation</li> <li>Added --demultiplex-chunk-size option</li> </ul>"},{"location":"changelog/#version-182","title":"Version 1.8.2","text":"<ul> <li>Added annotation (htseq) feature type as parameter</li> </ul>"},{"location":"changelog/#version-181","title":"Version 1.8.1","text":"<ul> <li>Fix a small bug when having the UMI before the BARCODE</li> </ul>"},{"location":"changelog/#version-180","title":"Version 1.8.0","text":"<ul> <li>Improved the unit-tests</li> <li>Make input data a positional arguments in extra scripts</li> </ul>"},{"location":"changelog/#version-179","title":"Version 1.7.9","text":"<ul> <li>--ref-map is not a required parameter now when --disable_mapping is used</li> <li>Create dataset and compute saturation steps are only performed if the annotated file is present</li> <li>Fixed bug in merge_fastq.py</li> <li>Fixed bugs in st_qa.py</li> </ul>"},{"location":"changelog/#version-178","title":"Version 1.7.8","text":"<ul> <li>Updated docs</li> <li>Added flags to skip (trimming, mapping and annotation)</li> <li>Fixed a bug in the compute saturation option</li> <li>Restored the unittests</li> <li>Improvements when dealing with ambiguous and no_feature genes</li> </ul>"},{"location":"changelog/#version-176","title":"Version 1.7.6","text":"<ul> <li>Ported to Python 3</li> </ul>"},{"location":"changelog/#version-173","title":"Version 1.7.3","text":"<ul> <li>Fixed a small bug when soring the output of taggd</li> <li>Improved the st_qa.py script</li> </ul>"},{"location":"changelog/#version-172","title":"Version 1.7.2","text":"<ul> <li>Removed parallel code for un-necessary parts</li> <li>Added option to provide saturation points</li> </ul>"},{"location":"changelog/#version-165","title":"Version 1.6.5","text":"<ul> <li>Improved the st_qa.py script</li> <li>Few small improvements in the annotation step</li> <li>Added support to use a transcriptome (--transcriptome)</li> </ul>"},{"location":"changelog/#version-162","title":"Version 1.6.2","text":"<ul> <li>Fixed a error in the parameters</li> </ul>"},{"location":"changelog/#version-161","title":"Version 1.6.1","text":"<ul> <li>Added option to disable the barcode demultiplexing step</li> <li>Added option to disable the UMI filtering step</li> </ul>"},{"location":"changelog/#version-160","title":"Version 1.6.0","text":"<ul> <li>Made the parsing of unique UMIs gene by gene and parallel</li> <li>AdjacentBi is now the default method for UMI counting</li> <li>Made the trimming function output the trimmed R2 in BAM format with the barcode and UMI</li> <li>Made the mapping function works with a BAM file as input (latest STAR release)</li> <li>Made the annotation function parallel</li> <li>Made the quality step parallel</li> <li>Improvements in speed and memory (constant memory use)</li> <li>The STAR genome loading strategy can be now set</li> <li>Added an affinity based method to cluster UMIs</li> <li>Added option to set the STAR BAM sort memory limit</li> </ul>"},{"location":"changelog/#version-151","title":"Version 1.5.1","text":"<ul> <li>Fixed a bug that will output the matrix of counts inverted</li> </ul>"},{"location":"changelog/#version-150","title":"Version 1.5.0","text":"<ul> <li>st_qa.py generate expression heatmap plots</li> <li>Fixed a minor bug in the computation of the saturation curves</li> <li>adjust_matrix_coordinates now does not update the coordinates by default</li> <li>adjust_matrix_coordiantes works with the latest ST Spot detector format</li> <li>small updates in the fastq merging script</li> <li>relaxed a bit the restriction checks for some parameters</li> </ul>"},{"location":"changelog/#version-145","title":"Version 1.4.5","text":"<ul> <li>Added extra scripts:<ul> <li>merge_bcl : merge BCL files based in patterns</li> <li>filter_gene_type_matrix : filter gene in output data based on Ensembl gene types</li> </ul> </li> <li>Bumped Pysam and HTSeq</li> </ul>"},{"location":"changelog/#version-141","title":"Version 1.4.1","text":"<ul> <li>Small update to make the PIP installation more robust</li> </ul>"},{"location":"changelog/#version-140","title":"Version 1.4.0","text":"<ul> <li>Small update to make the PIP installation more robust</li> </ul>"},{"location":"changelog/#version-135","title":"Version 1.3.5","text":"<ul> <li>Optimized the counting of UMIs by strand, start-pos and offset</li> </ul>"},{"location":"changelog/#version-134","title":"Version 1.3.4","text":"<ul> <li>Fixed a typo in one of the parameter that caused the pipeline to not run</li> </ul>"},{"location":"changelog/#version-133","title":"Version 1.3.3","text":"<ul> <li>Disabled spliced alignments by default</li> </ul>"},{"location":"changelog/#version-132","title":"Version 1.3.2","text":"<ul> <li>Optimized the mapping and annotation steps</li> </ul>"},{"location":"changelog/#version-131","title":"Version 1.3.1","text":"<ul> <li>Homopolymer miss-matches is a parameter now</li> <li>Removing now also PolyNs (parameter)</li> </ul>"},{"location":"changelog/#version-130","title":"Version 1.3.0","text":"<ul> <li>Added more methods to cluster UMIs</li> <li>Optimized the UMI counting algorithm</li> <li>Optimized the memory use</li> </ul>"},{"location":"changelog/#version-126","title":"Version 1.2.6","text":"<ul> <li>Take into account soft-clipped bases when computing start/end positions</li> </ul>"},{"location":"changelog/#version-125","title":"Version 1.2.5","text":"<ul> <li>Changed the limit range of some parameters</li> </ul>"},{"location":"changelog/#version-124","title":"Version 1.2.4","text":"<ul> <li>Fixed small bugs</li> <li>Small improvements in st_qa.py and convertEnsemblToNames.py</li> </ul>"},{"location":"changelog/#version-123","title":"Version 1.2.3","text":"<ul> <li>Bumped TaggD version</li> <li>Added more stats to the dataset output</li> <li>Added scripts to compute stats</li> <li>Added new option for TaggD</li> </ul>"},{"location":"changelog/#version-122","title":"Version 1.2.2","text":"<ul> <li>Fixed bugs in convertEnsemblToNames</li> <li>Added some parameters for TaggD demultiplexing</li> <li>Bumped version of TaggD</li> </ul>"},{"location":"changelog/#version-121","title":"Version 1.2.1","text":"<ul> <li>Made homopolymers filters enabled by default</li> <li>Added a test dataset to the docs</li> </ul>"},{"location":"changelog/#version-120","title":"Version 1.2.0","text":"<ul> <li>Fixed a small bug in the deletion of the tmp folder</li> </ul>"},{"location":"changelog/#version-117","title":"Version 1.1.7","text":"<ul> <li>Make sure to remove tmp files even if an error happens</li> </ul>"},{"location":"changelog/#version-116","title":"Version 1.1.6","text":"<ul> <li>Fixed bug that would leave some files in /tmp</li> <li>Allowed mis-matches when removing adaptors is now 2</li> </ul>"},{"location":"changelog/#version-115","title":"Version 1.1.5","text":"<ul> <li>Removed some un-necessary parameters</li> </ul>"},{"location":"changelog/#version-111","title":"Version 1.1.1","text":"<ul> <li>Simplified the two pass mode</li> </ul>"},{"location":"changelog/#version-110","title":"Version 1.1.0","text":"<ul> <li>Added flag to discard reads mapping to anti-sense strand</li> <li>Parameters for GC content filter instead of using the same value as AT content filter</li> <li>Fixed a small bug in the logging of some parameters</li> </ul>"},{"location":"changelog/#version-104","title":"Version 1.0.4","text":"<ul> <li>When removing adaptors (homopolymers streches) allow to up to 3 missmatches</li> <li>Added GC content filter (same % as AT content)</li> </ul>"},{"location":"changelog/#version-103","title":"Version 1.0.3","text":"<ul> <li>Fixed a minor bug in the counting of UMIs or - strand</li> </ul>"},{"location":"changelog/#version-102","title":"Version 1.0.2","text":"<ul> <li>If no temp folder is given a new unique one is created on top of the execution folder</li> <li>integrate createDataset.py into the code of the pipeline</li> <li>Adjusted some parameters names and descriptions (no UMI is default)</li> <li>Added sliding window when counting unique molecules</li> <li>Added support to bzip</li> </ul>"},{"location":"changelog/#version-101","title":"Version 1.0.1","text":"<ul> <li>Fixed small bug in the parsing of the umi quality parameter</li> </ul>"},{"location":"changelog/#version-100","title":"Version 1.0.0","text":"<ul> <li>Added option to check for UMI quality</li> <li>Optimized the UMI template check code</li> <li>Optimized how the unique molecules are counted</li> <li>Better stats for the quality filter step</li> <li>Updated convertEnsemblToNames script</li> <li>Updated stringdocs</li> </ul>"},{"location":"changelog/#version-099","title":"Version 0.9.9","text":"<ul> <li>Small bug fixes</li> </ul>"},{"location":"changelog/#version-096","title":"Version 0.9.6","text":"<ul> <li>Fixed a bug with the non ambiguous option</li> <li>Fix a bug in the saturation computation</li> </ul>"},{"location":"changelog/#version-095","title":"Version 0.9.5","text":"<ul> <li>When a R2 is trimmed its correspondant R1 is trimmed as well</li> </ul>"},{"location":"changelog/#version-094","title":"Version 0.9.4","text":"<ul> <li>Fixed a stupid bug in the compute saturation option</li> </ul>"},{"location":"changelog/#version-093","title":"Version 0.9.3","text":"<ul> <li>Changed the rRNA filter so the BAM output does not need to be sorted</li> </ul>"},{"location":"changelog/#version-092","title":"Version 0.9.2","text":"<ul> <li>Fixed a bug in the parsing of parameters</li> </ul>"},{"location":"changelog/#version-091","title":"Version 0.9.1","text":"<ul> <li>Fixed a small bug with the location of discarded files</li> </ul>"},{"location":"changelog/#version-090","title":"Version 0.9.0","text":"<ul> <li>Replaced JSON for data frame in the output format</li> <li>Replaced python gzip for system call (faster)</li> <li>Changed the logic of how the filenames are stored and handled</li> </ul>"},{"location":"changelog/#version-089","title":"Version 0.8.9","text":"<ul> <li>Improved the error messages and error handling</li> </ul>"},{"location":"changelog/#version-088","title":"Version 0.8.8","text":"<ul> <li>Removed barcodes IDs from the output file</li> </ul>"},{"location":"changelog/#version-087","title":"Version 0.8.7","text":"<ul> <li>Updated comments, manual and license</li> <li>Small improvements</li> </ul>"},{"location":"changelog/#version-085","title":"Version 0.8.5","text":"<ul> <li>Fixed a bug in the computation of saturation curves</li> </ul>"},{"location":"changelog/#version-084","title":"Version 0.8.4","text":"<ul> <li>Added a normal hash with INT keys to increase speed and reduce memory</li> <li>Using the gene_id for annotation again</li> </ul>"},{"location":"changelog/#version-083","title":"Version 0.8.3","text":"<ul> <li>Added parameter for strandness in annotation (yes by default)</li> <li>Simplified a bit the quality trimming step (do not account for user input trimmed bases)</li> </ul>"},{"location":"changelog/#version-082","title":"Version 0.8.2","text":"<ul> <li>Added stats for annotated reads</li> <li>Replaced shelve dict for sqldict</li> <li>Fixed some small bugs in the annotation</li> </ul>"},{"location":"changelog/#version-081","title":"Version 0.8.1","text":"<ul> <li>Removed the pair mode keep option</li> <li>Removed un-neccessary pair mode and mapped checks   after alignment</li> </ul>"},{"location":"changelog/#version-080","title":"Version 0.8.0","text":"<ul> <li>Added option to do the STAR 2 pass mode</li> <li>Removed option to run pipeline without IDs</li> <li>Speed improvements</li> <li>Perform demultiplex after mapping</li> <li>No attaching the barcode to reverse reads</li> <li>Removing some parameters</li> <li>Some improvements in stDataPlotter</li> <li>Option to use BAM format</li> <li>Removed annotation filtering step</li> <li>Removed forward trimming parameters</li> <li>Output gene names even with ENSEMBL</li> </ul>"},{"location":"changelog/#version-077","title":"Version 0.7.7","text":"<ul> <li>Small memory improvements</li> <li>Updates in plotting script</li> </ul>"},{"location":"changelog/#version-076","title":"Version 0.7.6","text":"<ul> <li>End coordinates now contain the whole read length</li> <li>Make annotation strand aware (reverse)</li> <li>Updated to STAR 2.5</li> </ul>"},{"location":"changelog/#version-075","title":"Version 0.7.5","text":"<ul> <li>Fixed a small bug</li> </ul>"},{"location":"changelog/#version-074","title":"Version 0.7.4","text":"<ul> <li>Added some memory improvements</li> </ul>"},{"location":"changelog/#version-073","title":"Version 0.7.3","text":"<ul> <li>Added parameters for inverse trimming</li> <li>Memory and speed optimizations in createDatasets</li> <li>Added option for low_memory use</li> </ul>"},{"location":"changelog/#version-072","title":"Version 0.7.2","text":"<ul> <li>Added unique genes to saturation points</li> <li>Added option to keep non-annotated reads</li> </ul>"},{"location":"changelog/#version-071","title":"Version 0.7.1","text":"<ul> <li>Fixed some small bugs</li> </ul>"},{"location":"changelog/#version-070","title":"Version 0.7.0","text":"<ul> <li>Fixed a bug in the saturation points</li> <li>Removed counttrie as option for clustering</li> <li>Updated and improved CTTS scripts</li> <li>Updated datfa plotter color list</li> </ul>"},{"location":"changelog/#version-069","title":"Version 0.6.9","text":"<ul> <li>Fixed a bug in the saturation points</li> </ul>"},{"location":"changelog/#version-068","title":"Version 0.6.8","text":"<ul> <li>Improved speed and memory in createDatasets</li> <li>Changed saturation points to fixed values that grow exp</li> <li>Improved speed in computation of saturation points</li> <li>Small bug fixes</li> <li>Upgraded json2Scatter with many improvements</li> <li>Rename json2scatter to stDataPlotter</li> </ul>"},{"location":"changelog/#version-067","title":"Version 0.6.7","text":"<ul> <li>Fixed a bug in the hierarchical clustering</li> <li>Added the input parameter to qa_stats</li> <li>Append experiment name to output files</li> <li>Added option to compute saturation points</li> <li>Added tool to plot stdata and clusters with aligned image</li> </ul>"},{"location":"changelog/#version-066","title":"Version 0.6.6","text":"<ul> <li>Fixed a bug in the hierarchical clustering</li> <li>Fixed a bug in the printed stats</li> </ul>"},{"location":"changelog/#version-065","title":"Version 0.6.5","text":"<ul> <li>Fixed a bug in retrieving the version of the software</li> <li>Added time stamps in different steps</li> <li>Added a UMI template quality filter</li> </ul>"},{"location":"changelog/#version-064","title":"Version 0.6.4","text":"<ul> <li>Fixed a bug in counttrie clustering method</li> <li>Improved sorting of molecular barcodes prior clustering</li> <li>Added hiearachical clustering option</li> </ul>"},{"location":"changelog/#version-063","title":"Version 0.6.3","text":"<ul> <li>Removed reads.json</li> <li>Added qa_stats.json to the output</li> <li>Restored old versioning system</li> <li>Removed hadoop related stuff</li> <li>Added support for gziped input files</li> </ul>"},{"location":"changelog/#version-062","title":"Version 0.6.2","text":"<ul> <li>Improved the log a bit</li> <li>Added parameters for max,min intron size and max gap size</li> </ul>"},{"location":"changelog/#version-061","title":"Version 0.6.1","text":"<ul> <li>Fixed some bugs in the prefix trie</li> </ul>"},{"location":"changelog/#version-059","title":"Version 0.5.9","text":"<ul> <li>Added an option to find molecular barcodes clusters using a prefix trie</li> </ul>"},{"location":"changelog/#version-058","title":"Version 0.5.8","text":"<ul> <li>Fixed a bug in the function to retrieve the pipeline version</li> </ul>"},{"location":"changelog/#version-057","title":"Version 0.5.7","text":"<ul> <li>Fixed a bug with --disable-multimap option</li> </ul>"},{"location":"changelog/#version-056","title":"Version 0.5.6","text":"<ul> <li>Fixed a typo in a parameter</li> <li>Fixed a bug that caused some parameters to not work</li> </ul>"},{"location":"changelog/#version-055","title":"Version 0.5.5","text":"<ul> <li>Added some extra debugging info in createDatasets</li> <li>Output the read name in the BED output file</li> <li>Changed --allowed-kimera for --allowed-kmer</li> <li>Added version as parameter and log message</li> </ul>"},{"location":"changelog/#version-054","title":"Version 0.5.4","text":"<ul> <li>Added parameter to disable soft clipping in mapping</li> <li>Disable softclipping in rRNA filter</li> <li>Make sure that discarded reads after rRNA filter are replaced by Ns</li> <li>Improved stats info a bit</li> </ul>"},{"location":"changelog/#version-053","title":"Version 0.5.3","text":"<ul> <li>Bumped Taggd to 0.2.2</li> </ul>"},{"location":"changelog/#version-052","title":"Version 0.5.2","text":"<ul> <li>Fixed a bug in the rRNA filter that would cause to not discard rRNA mapped reads</li> </ul>"},{"location":"changelog/#version-051","title":"Version 0.5.1","text":"<ul> <li>Added check when UMI is the same as barcode</li> <li>Added more stats</li> <li>Added percentiles distributiosn stats for createDAtaset</li> <li>Added support for BAM and SAM (not functional now)</li> <li>Added option to disable multiple aligned reads</li> <li>Fixed a bug in the bed file</li> </ul>"},{"location":"changelog/#version-050","title":"Version 0.5.0","text":"<ul> <li>Added AT content filter in quality trimming</li> <li>Added min mapped length filter after mapping</li> <li>Make sure one of the multiple aligned reads is set as not multiple aligned so it can be annotated</li> <li>Discard the other multiple aligned reads after mapping</li> <li>Disable sorting</li> <li>Restored back to use gene_id as column for annotation</li> </ul>"},{"location":"changelog/#version-049","title":"Version 0.4.9","text":"<ul> <li>Changed naming convention</li> <li>Added support for normal RNA analysis</li> </ul>"},{"location":"changelog/#version-048","title":"Version 0.4.8","text":"<ul> <li>Improved STAR configuration</li> <li>Added mapping post processing to filter out and adjust reversed reads</li> <li>Changed to use gene_name for annotation</li> <li>Fixed some bugs and some improvements</li> <li>Fixed bugs in the trimming</li> </ul>"},{"location":"changelog/#version-047","title":"Version 0.4.7","text":"<ul> <li>Improved stats</li> <li>Fixed a bug that would remove original input files</li> <li>Added a script to convert ENSEMBL ids to gene names</li> </ul>"},{"location":"changelog/#version-046","title":"Version 0.4.6","text":"<ul> <li>Fixed a bug that would not compute the number of discarded reads when using molecular barcodes</li> </ul>"},{"location":"changelog/#version-045","title":"Version 0.4.5","text":"<ul> <li>Fixed a bug in the barcodes JSON output</li> </ul>"},{"location":"changelog/#version-044","title":"Version 0.4.4","text":"<ul> <li>Fixed a bug in the molecular barcodes algorithm</li> <li>Fixed a bug that would keep the original fastq reads in the system</li> <li>Update taggd version</li> </ul>"},{"location":"changelog/#version-043","title":"Version 0.4.3","text":"<ul> <li>Small improvements with error checking and log in the mapping</li> <li>Fixed a bug that would remove the file after filtering annoted reads</li> <li>Make the sorting by name instead by position due to a bug in htseq-count</li> </ul>"},{"location":"changelog/#version-042","title":"Version 0.4.2","text":"<ul> <li>Fixed a bug in the capture of parameters</li> </ul>"},{"location":"changelog/#version-041","title":"Version 0.4.1","text":"<ul> <li>Improved the logs</li> <li>Fixed few bugs</li> </ul>"},{"location":"changelog/#version-040","title":"Version 0.4.0","text":"<ul> <li>Added back taggd</li> <li>Added BED file to output</li> <li>Added STAR</li> <li>Optimized workflow</li> <li>do rRNA filter first</li> <li>Optimized annotation</li> <li>Optimized trimming</li> <li>Output reads do not contain duplicates</li> </ul>"},{"location":"changelog/#version-039","title":"Version 0.3.9","text":"<ul> <li>Allowing molecular barcodes to be before the barcodes</li> </ul>"},{"location":"changelog/#version-038","title":"Version 0.3.8","text":"<ul> <li>Added back findIndexes</li> </ul>"},{"location":"changelog/#version-037","title":"Version 0.3.7","text":"<ul> <li>Removed cutadapt dependency</li> </ul>"},{"location":"changelog/#version-036","title":"Version 0.3.6","text":"<ul> <li>Fixed a bug in the installation</li> </ul>"},{"location":"changelog/#version-035","title":"Version 0.3.5","text":"<ul> <li>Added options to remove PolyC fix bugs in adaptors removal</li> </ul>"},{"location":"changelog/#version-034","title":"Version 0.3.4","text":"<ul> <li>Added test for STAR and STAR binary to dependencies</li> <li>Added TAGGD and removed findIndexes</li> <li>Improved install script</li> <li>Added options to remove adaptors (PolyA, PolyT and PolyG)</li> <li>Exchanged Bowtie as primary mapper with STAR.</li> </ul>"},{"location":"changelog/#version-033","title":"Version 0.3.3","text":"<ul> <li>Added option to keep files with discarded reads/barcodes</li> <li>Internal refactoring and optimization</li> </ul>"},{"location":"changelog/#version-032","title":"Version 0.3.2","text":"<ul> <li>Outputted reads JSON now only has the portion of the read that was used to map</li> <li>Cutadapt is integrated but only using the quality trimming for now</li> <li>Internal refactoring and optimizations</li> </ul>"},{"location":"changelog/#version-031","title":"Version 0.3.1","text":"<ul> <li>Added small unit-test for molecular barcodes</li> <li>Added more molecular barcodes algorithms (using a naive one for now)</li> <li>Fixed small issues in JSON parsing libraries</li> </ul>"},{"location":"changelog/#version-030","title":"Version 0.3.0","text":"<ul> <li>Rewrite createDatasets.py</li> <li>Clean up repository and deprecated files</li> <li>Change the unit-test library and structure</li> <li>Refactor the unit-test (use pipeline API instead of command line calls)</li> <li>Ensure unit-test remove tmp files when failing</li> <li>Add better error handling</li> <li>Add unit-test for Molecular Barcodes</li> <li>Add Molecular Barcodes functionality</li> <li>General refactor and clean up</li> <li>Add invoke options (clean, build, install)</li> <li>Fix an important bug in createDatasets that caused incorrect   computation of reads counts</li> </ul>"},{"location":"changelog/#version-025","title":"Version 0.2.5","text":"<ul> <li>Improved installers</li> <li>Small bug fixes</li> <li>Added basic uni-test to do a run of the pipeline</li> </ul>"},{"location":"changelog/#version-024","title":"Version 0.2.4","text":"<ul> <li>Some optimizations and bug fixes</li> </ul>"},{"location":"changelog/#version-023","title":"Version 0.2.3","text":"<ul> <li>Fixed a error with new version of HTSeq-count that will discard more reads</li> </ul>"},{"location":"changelog/#version-022","title":"Version 0.2.2","text":"<ul> <li>Added extra parameters</li> <li>Fixed some typos</li> <li>Fixed a bug that caused to remove some bases from the barcode ID in the rw reads</li> </ul>"},{"location":"changelog/#version-021","title":"Version 0.2.1","text":"<ul> <li>code refactored and modularized</li> <li>add argparse for parameters parsing</li> <li>add API for Amazon EMR and terminal version</li> <li>better error handling</li> <li>optimized code</li> <li>new version of FindIndexes</li> <li>remove dependencies</li> <li>added proper installers and documentation</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/jfnavarro/st_pipeline/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>ST Pipeline could always use more documentation, whether as part of the official ST Pipeline docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/jfnavarro/st_pipeline/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started","text":"<p>Ready to contribute? Here's how to set up <code>ST Pipeline</code> for local development.</p> <ol> <li>Fork the <code>ST Pipeline</code> repo on GitHub.</li> <li>Clone your fork locally</li> </ol> <pre><code>git clone git@github.com:jfnavarro/st_pipeline.git\n</code></pre> <ol> <li>Ensure poetry is installed.</li> <li>Ensure STAR and samtools are installed.</li> <li>Install dependencies and start your virtualenv:</li> </ol> <pre><code>poetry install -E test -E doc -E dev\n</code></pre> <p>Note that you can use your own Python environment (e.g Anaconda) by changing the default behaviour in poetry with this command:</p> <pre><code>poetry config virtualenvs.create false\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with pytest:</li> </ol> <pre><code>poetry run pytest\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.10, 3.11 and 3.12. Check    https://github.com/jfnavarro/st_pipeline/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#testing","title":"Testing","text":"<p>You can run the tests with pytest:</p> <pre><code>poetry run pytest\n</code></pre> <p>Replace test_your_module.py with the actual name of your test file.</p>"},{"location":"contributing/#makefile","title":"Makefile","text":"<p>A <code>makefile</code> is included in the repo with the following actions:</p> <p>To run formatting tools</p> <pre><code>make format\n</code></pre> <p>To run linting tools</p> <pre><code>make lint\n</code></pre> <p>To run the tests</p> <pre><code>make unittet\n</code></pre> <p>To run the tests with coverage</p> <pre><code>make coverage\n</code></pre> <p>To clean the temporary files and cache</p> <pre><code>make clean\n</code></pre>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Make sure you have updated the version in <code>pyproject.toml</code> and <code>stpipeline/version.py</code>. Then run:</p> <pre><code>git tag &lt;version&gt; -m \"message\"\ngit push --tags\n</code></pre> <p>GitHub Actions will then create a release and publish documentation if tests pass.</p> <p>You can also create the documentation manually by running:</p> <pre><code>poetry run mkdocs build\n</code></pre>"},{"location":"contributing/#publish-package","title":"Publish package","text":"<p>Ensure that you have configured your PyPi tokens.</p> <pre><code>poetry config repositories.testpypi https://test.pypi.org/legacy/\npoetry config repositories.pypi https://upload.pypi.org/legacy/\n</code></pre> <p>and</p> <pre><code>poetry config pypi-token.pypi YOUR_PYPI_API_TOKEN\npoetry config pypi-token.testpypi YOUR_TEST_PYPI_API_TOKEN\n</code></pre> <p>Then run:</p> <pre><code>poetry build\npoetry publish -r test-pypi # optional\npoetry publish\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>Python 3.10, 3.11 or 3.12 is required.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>The ST Pipeline requires STAR installed in the system (minimum version 2.5.4 if you use a ST Pipeline version &gt;= 1.6.0). The ST Pipeline requires samtools installed in the system.</p> <p>If you use anaconda you can install STAR and samtools with:</p> <pre><code>conda install -c bioconda star samtools\n</code></pre> <p>The ST Pipeline needs a computer with at least 32GB of RAM (depending on the size of the genome) and 8 cpu cores.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>The ST Pipeline depends on some Python packages that will be automatically installed during the installation process. You can see them in the file <code>requirements.txt</code></p>"},{"location":"installation/#from-source","title":"From source","text":"<p>The source for <code>ST Pipeline</code> can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>git clone https://github.com/jfnavarro/stpipeline\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/jfnavarro/stpipeline/tarball/master\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p>"},{"location":"installation/#using-poetry","title":"Using Poetry","text":"<p>If you don't have Poetry installed you can use the following command:</p> <pre><code>curl -sSL https://install.python-poetry.org | python -\n</code></pre> <p>Install the package:</p> <pre><code>poetry install\n</code></pre> <p>Now you can run the ST Pipeline:</p> <pre><code>poetry run st_pipeline_run --help\n</code></pre>"},{"location":"installation/#using-pip","title":"Using Pip","text":"<p>If you don't have pip installed, this Python installation guide can guide you through the process.</p> <p>Install the package:</p> <pre><code>pip install .\n</code></pre> <p>You can also use the official PyPy repositories:</p> <pre><code>pip install stpipeline\n</code></pre> <p>Now you can run ST Pipeline:</p> <pre><code>st_pipeline_run --help\n</code></pre>"},{"location":"installation/#using-docker","title":"Using Docker","text":"<p>Before installing, ensure that Docker is installed in your environment.</p> <p>First, build a Docker image:</p> <pre><code>docker buildx build --platform linux/amd64 -t stpipeline .\n</code></pre> <p>Then, you can run ST Pipeline using Docker:</p> <p>To run <code>ST Pipeline</code> commands:</p> <pre><code>docker run --rm stpipeline st_pipeline_run --help\n</code></pre>"},{"location":"installation/#using-anaconda","title":"Using Anaconda","text":"<p>Before installing, ensure you have either Anaconda or Miniconda installed in your environment.</p> <p>First, create the environment:</p> <pre><code>conda env create -n stpipeline python=3.10\n</code></pre> <p>Then, activate the environment:</p> <pre><code>conda activate stpipeline\n</code></pre> <p>Install the package:</p> <pre><code>pip install .\n</code></pre> <p>Now you can run ST Pipeline:</p> <pre><code>st_pipeline_run --help\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#usage","title":"Usage","text":""},{"location":"usage/#input-format","title":"Input format","text":"<p>The following files/parameters are commonly required:</p> <ul> <li>FASTQ files (Read 1 containing the barcode and the UMI and read 2 containing the genomic sequence)</li> <li>A genome index generated with STAR</li> <li>An annotation file in GTF or GFF3 format (optional when using a transcriptome)</li> <li>A file containing the barcodes and array coordinates (look at the folder \"ids\" to use it as a reference). Basically this file contains 3 columns (BARCODE, X and Y), so if you provide this file with barcodes identifying cells (for example), the ST pipeline can be used for single cell data. This file is also optional if the data is not barcoded (for example RNA-Seq data).</li> <li>A name for the dataset</li> </ul> <p>The ST pipeline has multiple parameters mostly related to trimming, mapping, demultiplexing and annotation but generally the default values are good enough.</p> <p>The input FASTQ files can be given in gzip/bzip format as well.</p> <p>Note that the minimum read length is dependant on the type of kit used, and should be adjusted accordingly, i.e. a 150bp kit should have a different minimum read length than a 75bp kit.</p> <p>Soft clipping is also not recommended when using the 75bp kit, due to the shorter length.</p> <p>The UMI filter can be used for array batches 1000L6 and earlier. It is not recommended to use it for array batches 1000L7 and newer as the UMI in these arrays is fully randomised.</p> <p>The output of the ST Pipeline is a counts matrix (TSV) and a log file with status and useful information.</p> <p>You can see a full description of the parameters typing <code>st_pipeline_run --help</code> after you have installed the ST pipeline.</p> <pre><code>st_pipeline_run [options] fastq_file_fw fastq_file_rv\n\n  fastq_file_fw\n    Read_1 containing the spatial barcodes and UMIs for each sequence.\n\n  fastq_file_rv\n    Read_2 containing the gene sequence corresponding to the sequence in\n    Read_1.\n\n  -h, --help            show this help message and exit\n  --ids [FILE]          Path to the file containing the map of barcodes to the\n                        array coordinates\n  --ref-map [FOLDER]    Path to the folder with the STAR index for the genome\n                        that you want to use as reference\n  --ref-annotation [FILE]\n                        Path to the reference annotation file (GTF or GFF\n                        format is required) to be used to annotated the mapped\n                        reads\n  --expName [STRING]    Name of the dataset (The output files will prepend\n                        this name)\n  --contaminant-index [FOLDER]\n                        Path to the folder with a STAR index with a\n                        contaminant genome reference. Reads will be filtered\n                        using the specified genome and mapping reads will be\n                        discarded\n  --no-clean-up         Do not remove temporary/intermediary files (useful for\n                        debugging)\n  --verbose             Show extra information on the log file\n  --threads [INT]       Number of threads to use (default: 4)\n  --bin-path [FOLDER]   Path to folder where binary executables are present\n                        (system path by default)\n  --log-file [STR]      Name of the file that we want to use to store the logs\n                        (default output to screen)\n  --output-folder [FOLDER]\n                        Path of the output folder\n  --temp-folder [FOLDER]\n                        Path of the location for temporary files\n  --keep-discarded-files\n                        Keep files with discarded reads in every step\n  --qual-64             Use phred-64 quality instead of phred-33(default) in\n                        the quality trimming step\n  --min-length-qual-trimming [INT]\n                        Minimum length of the reads after trimming, shorter\n                        reads will be discarded (default: 20)\n  --min-quality-trimming [INT]\n                        Minimum phred quality a base must have in order to be\n                        kept in the quality trimming step (default: 20)\n  --remove-polyA [INT]  Remove PolyA stretches of the given length from R2\n                        (Use 0 to disable it) (default: 10)\n  --remove-polyT [INT]  Remove PolyT stretches of the given length from R2\n                        (Use 0 to disable it) (default: 10)\n  --remove-polyG [INT]  Remove PolyG stretches of the given length from R2\n                        (Use 0 to disable it) (default: 10)\n  --remove-polyC [INT]  Remove PolyC stretches of the given length from R2\n                        (Use 0 to disable it) (default: 10)\n  --remove-polyN [INT]  Remove PolyN stretches of the given length from R2\n                        (Use 0 to disable it) (default: 10)\n  --homopolymer-mismatches [INT]\n                        Number of mismatches allowed when removing\n                        homopolymers (A, T, G, C or N) (default: 0)\n  --filter-AT-content [INT%]\n                        Discards reads whose number of A and T bases in total\n                        are more or equal than the percentage given as input\n                        (0-100) (default: 90)\n  --filter-GC-content [INT%]\n                        Discards reads whose number of G and C bases in total\n                        are more or equal the percentage given as input\n                        (0-100) (default: 90)\n  --mapping-rv-trimming [INT]\n                        Number of bases to trim in the reverse reads (R2) for\n                        the mapping step (5' end) (default: 0)\n  --inverse-mapping-rv-trimming [INT]\n                        Number of bases to trim in the reverse reads (R2) for\n                        the mapping step (3' end) (default: 0)\n  --disable-multimap    If activated, multiple aligned reads obtained during\n                        mapping will be all discarded. Otherwise the highest\n                        scored one will be kept\n  --disable-clipping    If activated, disable soft-clipping (local alignment)\n                        in the mapping step\n  --min-intron-size [INT]\n                        Minimum allowed intron size when searching for splice\n                        variants with STAR Splices alignments are disabled by\n                        default (=1) but to turn it on set this parameter to a\n                        bigger number, for example 10 or 20. (default: 1)\n  --max-intron-size [INT]\n                        Maximum allowed intron size when searching for splice\n                        variants with STAR Splices alignments are disabled by\n                        default (=1) but to turn it on set this parameter to a\n                        big number, for example 10000 or 100000. (default: 1)\n  --star-two-pass-mode  Activates the 2-pass mode in STAR to improve mapping\n                        accuracy\n  --star-genome-loading [STRING]\n                        Similar to the STAR option --genomeLoad. It allows to\n                        load the genome index into memory so it can easily be\n                        shared by other jobs to save loading time. Read the\n                        STAR manual for more info on this. (default:\n                        NoSharedMemory)\n  --star-sort-mem-limit STAR_SORT_MEM_LIMIT\n                        The maximum available RAM for sorting BAM during\n                        mapping with STAR. Default is 0 which means that it\n                        will be set to the genome index size\n  --demultiplexing-mismatches [INT]\n                        Number of allowed mismatches when demultiplexing the\n                        reads against the barcodes with TaggD (default: 2)\n  --demultiplexing-kmer [INT]\n                        KMer size to use when demultiplexing against the\n                        barcodes with TaggD (default: 6)\n  --demultiplexing-overhang [INT]\n                        Extra flanking bases added on each side of the barcode\n                        when demultiplexing against the barcodes with TaggD\n                        (default: 0)\n  --demultiplexing-start [INT]\n                        Start position of the IDs (Barcodes) in R1 (counting\n                        from 0) (default: 0)\n  --demultiplexing-metric [STRING]\n                        Distance metric to use for TaggD demultiplexing:\n                        Options: Subglobal, Levenshtein or Hamming (default:\n                        Subglobal)\n  --demultiplexing-multiple-hits-keep-one\n                        When multiple ambiguous hits with same score are found\n                        in the demultiplexing step, keep only one (random).\n  --demultiplexing-trim-sequences DEMULTIPLEXING_TRIM_SEQUENCES\n                        Trim the barcodes in the input file when doing\n                        demultiplexing. The input given is a list of tuples\n                        START END START END where START is the integer\n                        position of the first base (0 based) and END is the\n                        integer position of the last base (1 based). The final\n                        barcode will be obtained by combining all the\n                        sequences given in the input. This is useful when\n                        having a barcode composed of multiple sequences in the\n                        reador when the barcode needs to be trimmed out.\n                        Trimmng sequences can be given several times.\n  --demultiplexing-chunk-size [INT]\n                        Chunk size for parallel processing (number of reads assigned to each thread) (default: 10000)\n  --htseq-mode [STRING]\n                        Mode of annotation when using htseq-count. Modes =\n                        {union, intersection-nonempty(default), intersection-\n                        strict}\n  --htseq-no-ambiguous  When using htseq-count discard reads annotating\n                        ambiguous genes (default False)\n  --htseq-features HTSEQ_FEATURES [HTSEQ_FEATURES ...]\n                        Which feature types to use from the GTF/GFF file in the annotation.\n                        Can be given more than one type (default exon)\n  --strandness [STRING]\n                        What strandness mode to use when annotating with\n                        htseq-count [no, yes(default), reverse]\n  --include-non-annotated\n                        Do not discard un-annotated reads (they will be\n                        labeled __no_feature)\n  --umi-cluster-algorithm [STRING]\n                        Type of clustering algorithm to use when performing\n                        UMIs duplicates removal. Options = {naive,\n                        hierarchical, Affinity, Adjacent and\n                        AdjacentBi(default)} Note that for the affinity method\n                        the umi allowed mismatches parameter will be ignored.\n  --umi-allowed-mismatches [INT]\n                        Number of allowed mismatches (hamming distance) that\n                        UMIs of the same gene-spot must have in order to\n                        cluster together (default: 1)\n  --umi-start-position [INT]\n                        Position in R1 (base wise) of the first base of the\n                        UMI (starting by 0) (default: 18)\n  --umi-end-position [INT]\n                        Position in R1 (base wise) of the last base of the UMI\n                        (starting by 1) (default: 27)\n  --umi-filter          Enables the UMI quality filter based on the template\n                        given in --umi-filter-template\n  --umi-filter-template [STRING]\n                        UMI template (IUPAC nucleotide code) for the UMI\n                        filter, default = WSNNWSNNV\n  --umi-quality-bases [INT]\n                        Maximum number of low quality bases allowed in an UMI\n                        (default: 6)\n  --umi-counting-offset [INT]\n                        UMI count for each gene-spot combination is computed\n                        as the number of unique UMIs in each strand/start\n                        position. However some reads might have slightly\n                        different start positions due to amplification\n                        artifacts. This parameters allows to define an offset\n                        window from where to count unique UMIs. You can set it\n                        to a very high value +9999 to count unique UMIs for\n                        the whole gene (default: 250)\n  --compute-saturation  Performs a saturation curve computation by sub-\n                        sampling the annotated reads, computing unique UMIs\n                        and adding the stats to the log file (this can be used\n                        to plot saturation curves)\n  --saturation-points SATURATION_POINTS [SATURATION_POINTS ...]\n                        Saturation points for the saturation curve computation\n                        can be provided instead of using default values.\n                        Provide a list of values like this for example: 10000\n                        20000 50000 100000\n  --disable-trimming    Use this flag if you want to skip the trimming step\n  --disable-mapping     Use this flag if you want to skip the mapping step\n  --disable-annotation  Use this flag if you want to skip the annotation\n  --disable-barcode     Use this flag if you want to skip the barcode\n                        demultiplexing step\n  --disable-umi         Use this flag if you want to skip the UMI filtering\n                        step\n  --transcriptome       Use this flag if you want to use transcriptome instead\n                        of a genome, the gene tag will be obtained from the\n                        transcriptome file\n  --version             show program's version number and exit\n</code></pre>"},{"location":"usage/#example","title":"Example","text":"<p>An example run would be:</p> <pre><code>st_pipeline_run --expName test --ids ids_file.txt \\\n  --ref-map path_to_index --htseq-no-ambiguous --log-file log_file.txt --output-folder /home/me/results \\\n  --ref-annotation annotation_file.gtf --contaminant-index path_to_cont_index file1.fastq file2.fastq\n</code></pre>"},{"location":"usage/#visium","title":"Visium","text":"<p>To process Visium datasets it is recommended to use these options:</p> <pre><code>--demultiplexing-mismatches 1\n--demultiplexing-kmer 4\n--umi-allowed-mismatches 2\n--umi-start-position 16\n--umi-end-position 28\n</code></pre>"},{"location":"usage/#extra-tools","title":"Extra tools","text":"<p>The ST Pipeline ships many scripts that will be installed automatically and that can be very useful to pre/post process the data and general QC stats and plots.</p>"},{"location":"usage/#emsembl-ids","title":"Emsembl ids","text":"<p>If you used an Ensembl annotation file and you would like change the output file so it contains gene ids/names instead of Ensembl ids. You can use the script <code>convertEnsemblToNames</code> that comes with the ST Pipeline</p> <pre><code>convertEnsemblToNames --annotation path_to_annotation_file --output st_data_updated.tsv st_data.tsv\n</code></pre>"},{"location":"usage/#merge-demultiplexed-fastq-files","title":"Merge demultiplexed FASTQ files","text":"<p>If you used different indexes to sequence and need to merge the fastq files you can use the script <code>merge_fastq</code> that comes with the ST Pipeline</p> <pre><code>merge_fastq --run-path path_to_run_folder --out-path path_to_output --identifiers S1 S2 S3 S4\n</code></pre> <p>Where <code>--identifiers</code> will be strings that identify each demultiplexed sample.</p>"},{"location":"usage/#filter-out-genes-by-gene-type","title":"Filter out genes by gene type","text":"<p>If you want to remove from the dataset (matrix in TSV) genes corresponding to certain gene types (For instance to keep only protein_coding). You can do so with the script <code>filter_gene_type_matrix</code> that comes with the ST Pipeline</p> <pre><code>filter_gene_type_matrix --gene-types-keep protein-coding --annotation path_to_annotation_file stdata.tsv\n</code></pre> <p>You may include the parameter <code>--ensembl-ids</code> if your genes are represented as Emsembl ids instead.</p> <p>The value of <code>--gene-types-keep</code> must match the annotation file provided.</p>"},{"location":"usage/#remove-spots-from-dataset","title":"Remove spots from dataset","text":"<p>If you want to remove spots from a dataset (matrix in TSV) for instance to keep only spots inside the tissue. You can do so with the script <code>adjust_matrix_coordinates</code> that comes with the ST Pipeline</p> <pre><code>adjust_matrix_coordinates --outfile new_stdata.tsv --coordinates-file coordinates.txt stdata.tsv\n</code></pre> <p>Where <code>coordinates.txt</code> must be a tab delimited file with 6 columns:</p> <pre><code>orig_x orig_y new_x new_y new_pixel_x new_pixel_y\n</code></pre> <p>Only spots whose coordinates in the file will be kept and then optionally you can update the coordinates in the matrix choosing for the new array or pixel coordinates.</p>"},{"location":"usage/#quality-stats","title":"Quality stats","text":"<p>The ST Pipeline generate useful stats/QC information in the LOG file but if you want to obtain more detailed information about the quality of the data, you can run the following script:</p> <pre><code>st_qa stdata.tsv\n</code></pre> <p>If you want to perform quality stats on multiple datasets you can run:</p> <pre><code>multi_qa stdata1.tsv stadata2.tsv stdata3.tsv stdata4.tsv\n</code></pre> <p><code>multi_qa</code> generates violing plots, correlation plots/tables and more useful information and it allows to log the counts for the correlation analaysis.</p>"}]}